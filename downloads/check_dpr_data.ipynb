{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doc(doc:dict):\n",
    "    \n",
    "    data = doc[\"dataset\"]\n",
    "    q = doc[\"question\"]\n",
    "    a = doc[\"answers\"]\n",
    "    pos = len(doc[\"positive_ctxs\"])\n",
    "    neg = len(doc[\"negative_ctxs\"])\n",
    "    h_neg = len(doc[\"hard_negative_ctxs\"])\n",
    "    \n",
    "    print(f\"Dataset: {data} | Q: {q} | A: {a}\\npos:{pos} neg:{neg} hard neg:{h_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ['dt', 'q', 'a', 'p', 'n', 'hn']\n",
    "dk = ['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs']\n",
    "doc_keys = dict(zip(k, dk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./data/retriever/*train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/retriever/nq-train.json',\n",
       " './data/retriever/trivia-train.json',\n",
       " './data/retriever/nq-adv-hn-train.json',\n",
       " './data/retriever/curatedtrec-train.json',\n",
       " './data/retriever/webq-train.json',\n",
       " './data/retriever/squad1-train.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irdata_list = glob.glob(data_path)\n",
    "irdata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( irdata_list[0], \"r\",encoding='utf8') as fr:\n",
    "    docs = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs'])\n",
      "58880\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[0].keys()}\\n{len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 키값 내용\n",
    "  \n",
    "dataset : 데이터 셋 정보.위키 triviz, trec, squad.  \n",
    "question: 질문 문장 하나  \n",
    "answers: mrc 최종 결과. 이하 '답'이라 하겠음  \n",
    "positive_ctxs: 답이 포함되어 있고, bm25 상위 랭크에 걸리는 결과들  \n",
    "negative_ctxs: random인 것 같음. 검색 스코어 0  \n",
    "hard_negative_ctxs: 검색 스코어가 높은데 답은 없는 검색 결과  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datatext(idx, doc_key):\n",
    "    pprint.pprint(docs[idx][doc_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title_score는 rank1 만 1 이고, 나머진 0. 뭘 기준으로 하는지 모르겠음. \n",
    "논문에서는 모든 passage에 제목을 [SEP] 토큰으로 이어붙였다고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: nq_train_psgs_w100 | Q: when did roller derby first appear in the press | A: ['1922']\n",
      "pos:2 neg:50 hard neg:99\n",
      "'nq_train_psgs_w100'\n"
     ]
    }
   ],
   "source": [
    "check_doc(docs[idx])\n",
    "get_datatext(idx, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when did roller derby first appear in the press\n",
      "2\n",
      "{'passage_id': '11105240',\n",
      " 'score': 1000,\n",
      " 'text': 'three decades. Among these races was an 8.5-mile roller marathon '\n",
      "         'organized in 1908 by a group of Chicago rink owners, a 24-hour '\n",
      "         'endurance championship held in Milwaukee in 1913, and a 24-hour '\n",
      "         'banked track race held at Madison Square Garden in 1914. \"The New '\n",
      "         'York Times\" noted that the crowd at Madison Square Garden enjoyed '\n",
      "         \"the sudden sprints and spills during the event's preliminary races. \"\n",
      "         'The term \"derby\", meaning a race or multi-race event, appeared in '\n",
      "         'the press as early as 1922, when the \"Chicago Tribune\" announced and '\n",
      "         'reported on the results of two \"roller derby\" events held that year.',\n",
      " 'title': 'History of roller derby',\n",
      " 'title_score': 1}\n",
      "-----------\n",
      "{'passage_id': '11105240',\n",
      " 'score': 13.821991,\n",
      " 'text': 'three decades. Among these races was an 8.5-mile roller marathon '\n",
      "         'organized in 1908 by a group of Chicago rink owners, a 24-hour '\n",
      "         'endurance championship held in Milwaukee in 1913, and a 24-hour '\n",
      "         'banked track race held at Madison Square Garden in 1914. \"The New '\n",
      "         'York Times\" noted that the crowd at Madison Square Garden enjoyed '\n",
      "         \"the sudden sprints and spills during the event's preliminary races. \"\n",
      "         'The term \"derby\", meaning a race or multi-race event, appeared in '\n",
      "         'the press as early as 1922, when the \"Chicago Tribune\" announced and '\n",
      "         'reported on the results of two \"roller derby\" events held that year.',\n",
      " 'title': 'History of roller derby',\n",
      " 'title_score': 0}\n"
     ]
    }
   ],
   "source": [
    "doc_key = doc_keys[\"q\"]\n",
    "print(docs[idx][doc_key])\n",
    "doc_key = doc_keys[\"p\"]\n",
    "print(len(docs[idx][doc_key]))\n",
    "pprint.pprint(docs[idx][doc_key][0])\n",
    "print(\"-\"*11)\n",
    "pprint.pprint(docs[idx][doc_key][1])\n",
    "#check_doc(docs[idx])\n",
    "#(idx, \"positive_ctxs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### score 1000은 무엇인가?\n",
    "\n",
    "위 예시를 보면(nq_train, index 19 문서)  \n",
    "완전 동일한 title, text 인데 title_score, score가 서로 다름(1, 1000 : 0, 13.82)  \n",
    "아마 기존 데이터셋 정답인 경우와 아닌 경우로 나뉘는거 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서마다 pos, neg, hard neg 수가 다다름. 배치 생성시 instance를 다른 조합으로 여러개 만드나? 싶음  \n",
    "\n",
    "* 논문에서는 q-passgae 쌍 하나가 instance 하나. 배치 안에 다른 답을 가지고 in-batch negative sampling을 함.\n",
    "* q 하나에 여러 ctxs 들은 한번에 같이 쓰는건 아닐거 같고 그때 그떄 한쌍을 만드는게 아닐까? 하는 중"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['replace', 'me', 'by', 'any', 'text', 'you', \"'\", 'd', 'like', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "print(tokenizer.tokenize(text))\n",
    "#encoded_input = tokenizer(text, return_tensors='pt')\n",
    "#output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "text = docs[idx][doc_key][1][\"text\"]\n",
    "# whitespace 기준 100\n",
    "print(len(text.split(\" \")))\n",
    "# sub-word 기준 max len은 256. 코드에 기본값이 그렇게 들어가 있음\n",
    "print(len(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjb/miniconda3/envs/rerank/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer,ElectraModel\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "#tokenizer.tokenize(\"[CLS] 한국어 ELECTRA를 공유합니다. [SEP]\")\n",
    "#tokenizer.convert_tokens_to_ids(['[CLS]', '한국어', 'EL', '##EC', '##TRA', '##를', '공유', '##합니다', '.', '[SEP]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "text = docs[idx][doc_key][1][\"text\"]\n",
    "# whitespace 기준 100\n",
    "#print(len(text.split(\" \")))\n",
    "# sub-word 기준 max len은 256. 코드에 기본값이 그렇게 들어가 있음\n",
    "print(len(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 실행 command\n",
    "\n",
    "* zsh 버전  \n",
    "```python train_dense_encoder.py --train_datasets=\\[nq_train\\] dev_datasets=\\[nq_dev\\] train=biencoder_local```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rerank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c254622fc36917f92d8fc2abe99f88a7418b97289d0388c5753f81cf7e3259a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
