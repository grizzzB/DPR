{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_doc(doc:dict):\n",
    "    \n",
    "    data = doc[\"dataset\"]\n",
    "    q = doc[\"question\"]\n",
    "    a = doc[\"answers\"]\n",
    "    pos = len(doc[\"positive_ctxs\"])\n",
    "    neg = len(doc[\"negative_ctxs\"])\n",
    "    h_neg = len(doc[\"hard_negative_ctxs\"])\n",
    "    \n",
    "    print(f\"Dataset: {data} | Q: {q} | A: {a}\\npos:{pos} neg:{neg} hard neg:{h_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datatext(idx, doc_key):\n",
    "    pprint.pprint(docs[idx][doc_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ['dt', 'q', 'a', 'p', 'n', 'hn']\n",
    "dk = ['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs']\n",
    "doc_keys = dict(zip(k, dk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./data/retriever/*train.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/retriever/nq-train.json',\n",
       " './data/retriever/trivia-train.json',\n",
       " './data/retriever/nq-adv-hn-train.json',\n",
       " './data/retriever/curatedtrec-train.json',\n",
       " './data/retriever/webq-train.json',\n",
       " './data/retriever/squad1-train.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irdata_list = glob.glob(data_path)\n",
    "irdata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( irdata_list[5], \"r\",encoding='utf8') as fr:\n",
    "    docs = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset', 'question', 'answers', 'positive_ctxs', 'negative_ctxs', 'hard_negative_ctxs'])\n",
      "78713\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[0].keys()}\\n{len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 키값 내용\n",
    "  \n",
    "* dataset : 데이터 셋 정보.위키 triviz, trec, squad.  \n",
    "* question: 질문 문장 하나  \n",
    "* answers: mrc 최종 결과. 이하 '답'이라 하겠음  \n",
    "* positive_ctxs: 답이 포함되어 있고, bm25 상위 랭크에 걸리는 결과들  \n",
    "* negative_ctxs: random or 다른 q의 positive. 검색 스코어 0  \n",
    "* hard_negative_ctxs: 검색 스코어가 높은데 답은 없는 검색 결과  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title_score는 rank1 만 1 이고, 나머진 0. 뭘 기준으로 하는지 모르겠음. \n",
    "논문에서는 모든 passage에 제목을 [SEP] 토큰으로 이어붙였다고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "poslist=[]\n",
    "neglist=[]\n",
    "hneglist=[]\n",
    "for i, doc in enumerate(docs):\n",
    "    pos = len(doc[\"positive_ctxs\"])\n",
    "    neg = len(doc[\"negative_ctxs\"])\n",
    "    h_neg = len(doc[\"hard_negative_ctxs\"])\n",
    "    poslist.append(pos)\n",
    "    neglist.append(neg)\n",
    "    hneglist.append(h_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** max ****\n",
      "101\n",
      "50\n",
      "100\n",
      "**** min ****\n",
      "0\n",
      "50\n",
      "0\n",
      "**** mean ****\n",
      "5.272902824184061\n",
      "50.0\n",
      "95.55496550760357\n",
      "**** median ****\n",
      "2.0\n",
      "50.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "print(\"**** max ****\")\n",
    "print(np.max(poslist))\n",
    "print(np.max(neglist))\n",
    "print(np.max(hneglist))\n",
    "print(\"**** min ****\")\n",
    "print(np.min(poslist))\n",
    "print(np.min(neglist))\n",
    "print(np.min(hneglist))\n",
    "print(\"**** mean ****\")\n",
    "print(np.mean(poslist))\n",
    "print(np.mean(neglist))\n",
    "print(np.mean(hneglist))\n",
    "print(\"**** median ****\")\n",
    "print(np.median(poslist))\n",
    "print(np.median(neglist))\n",
    "print(np.median(hneglist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예시 둘러보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: squad1-train-psgs_w100 | Q: How many colleges for undergraduates are at Notre Dame? | A: ['five']\n",
      "pos:4 neg:50 hard neg:97\n",
      "'squad1-train-psgs_w100'\n"
     ]
    }
   ],
   "source": [
    "check_doc(docs[idx])\n",
    "get_datatext(idx, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many colleges for undergraduates are at Notre Dame?\n",
      "50\n",
      "{'psg_id': '1026879',\n",
      " 'score': 0,\n",
      " 'text': 'median income for a household in the city was $48,276, and the '\n",
      "         'median income for a family was $52,888. Males had a median income of '\n",
      "         '$36,214 versus $28,231 for females. The per capita income for the '\n",
      "         'city was $20,253. About 3.8% of families and 5.4% of the population '\n",
      "         'were below the poverty line, including 6.3% of those under age 18 '\n",
      "         'and 4.7% of those age 65 or over. Notable individuals who were born '\n",
      "         'in or have lived in Northglenn include: Northglenn, Colorado '\n",
      "         'Northglenn is a Home Rule Municipality in Adams and Weld counties in '\n",
      "         'the U.S. state of Colorado. As',\n",
      " 'title': 'Northglenn, Colorado',\n",
      " 'title_score': 0}\n",
      "-----------\n",
      "{'psg_id': '16050928',\n",
      " 'score': 0,\n",
      " 'text': 'Carlos Ramírez Suárez Carlos Ramírez Suárez (July 3, 1902 - August '\n",
      "         '30, 1978) was a Spanish lawyer, writer and chronicler of the island '\n",
      "         'of Gran Canaria. He was the fourth son of lawyer and journalist '\n",
      "         'Rafael Ramirez Doreste and his wife Dolores Suárez King. He went to '\n",
      "         'school at the college of Pedro Quevedo, in Castle Street Las Palmas '\n",
      "         'de Gran Canaria, and high school at the Colegio San Agustin in the '\n",
      "         'same city. For his law degree he studied at the Central University '\n",
      "         'of Madrid. During his time as an undergraduate, he attended the '\n",
      "         'Library of Ateneo de Madrid,',\n",
      " 'title': 'Carlos Ramírez Suárez',\n",
      " 'title_score': 0}\n"
     ]
    }
   ],
   "source": [
    "doc_key = doc_keys[\"q\"]\n",
    "print(docs[idx][doc_key])\n",
    "doc_key = doc_keys[\"n\"]\n",
    "print(len(docs[idx][doc_key]))\n",
    "pprint.pprint(docs[idx][doc_key][0])\n",
    "print(\"-\"*11)\n",
    "pprint.pprint(docs[idx][doc_key][1])\n",
    "#check_doc(docs[idx])\n",
    "#(idx, \"positive_ctxs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 탐색 노트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### score 1000은 무엇인가?\n",
    "\n",
    "위 예시를 보면(nq_train, index 19 문서)  \n",
    "완전 동일한 title, text 인데 title_score, score가 서로 다름(1, 1000 : 0, 13.82)  \n",
    "아마 기존 데이터셋 정답인 경우와 아닌 경우로 나뉘는거 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서마다 pos, neg, hard neg 수가 다다름. 배치 생성시 instance를 다른 조합으로 여러개 만드나? 싶음  \n",
    "\n",
    "* 논문에서는 q-passgae 쌍 하나가 instance 하나. 배치 안에 다른 답을 가지고 in-batch negative sampling을 함.\n",
    "* q 하나에 여러 ctxs 들은 한번에 같이 쓰는건 아닐거 같고 그때 그떄 한쌍을 만드는게 아닐까? 하는 중"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neg_ctxs, hard_ctxs, pos_ctxs 고찰\n",
    "* 딱 정해진건 없음. hard는 0인 경우도 많음. 많으면 99개씩 넣어둠.\n",
    "* squad, nq_train 만 neg_ctxs를 50개씩 채워둠(나머진 길이 0). 나머진 원래 데이터셋에서 QA 쌍만 제공하기 떄문이라함.\n",
    "* hard_negative들은 모든 dataset에 들어가 있음. 대략 100개에 가깝게 9x개를 채운 느낌. 샘플마다 0개인 건 껴있지만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['replace', 'me', 'by', 'any', 'text', 'you', \"'\", 'd', 'like', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "print(tokenizer.tokenize(text))\n",
    "#encoded_input = tokenizer(text, return_tensors='pt')\n",
    "#output = model(**encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "text = docs[idx][doc_key][1][\"text\"]\n",
    "# whitespace 기준 100\n",
    "print(len(text.split(\" \")))\n",
    "# sub-word 기준 max len은 256. 코드에 기본값이 그렇게 들어가 있음\n",
    "print(len(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjb/miniconda3/envs/rerank/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer,ElectraModel\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "#tokenizer.tokenize(\"[CLS] 한국어 ELECTRA를 공유합니다. [SEP]\")\n",
    "#tokenizer.convert_tokens_to_ids(['[CLS]', '한국어', 'EL', '##EC', '##TRA', '##를', '공유', '##합니다', '.', '[SEP]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "text = docs[idx][doc_key][1][\"text\"]\n",
    "# whitespace 기준 100\n",
    "#print(len(text.split(\" \")))\n",
    "# sub-word 기준 max len은 256. 코드에 기본값이 그렇게 들어가 있음\n",
    "print(len(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "print(l[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 실행 command\n",
    "\n",
    "* zsh 버전  \n",
    "```python train_dense_encoder.py --train_datasets=\\[nq_train\\] dev_datasets=\\[nq_dev\\] train=biencoder_local```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rerank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c254622fc36917f92d8fc2abe99f88a7418b97289d0388c5753f81cf7e3259a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
